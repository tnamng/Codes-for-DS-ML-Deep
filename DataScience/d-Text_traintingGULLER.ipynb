{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "\"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words) \n",
    "\n",
    "- min_df\n",
    "- token_pattern\n",
    "- ngram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)  # encoding: biến text thành \n",
    "print(\"bag_of_words: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    "bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=\"english\", ngram_range=(2, 2)).fit(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Hi there!\")  : cắt nhỏ câu thành LIST từ vựng và punctuaion \n",
    "sent_tokenize(đoạn văn)  --> câu \n",
    "regexp_tokenize() : tokenize a string or document based on a\n",
    "TweetTokenizer()  -->hastag weet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(word_tokenize(\"Hi there!\"))  --> dictinary từ:số lượng count object\n",
    "count.most_common(2) ---> 2 từ phổ biến\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing = lowercase, lấy root word, bỏ stopword\n",
    "\n",
    "from ntlk.corpus import stopwords\n",
    "text = \"\"\"The cat is in the box. The cat likes the box.\n",
    "The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] chỉ lấy alphabet character\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "'I really liked the movie!',\n",
    "'Awesome action scenes, but boring characters.',\n",
    "'The movie was awful! I hate alien films.',\n",
    "'Space is cool! I liked the movie.',\n",
    "'More space films, please!',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "# có thể updqte với new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]\n",
    "      \n",
    " Term frequency - inverse document frequency    \n",
    "      \n",
    "      \n",
    "      Each corpus may have shared words beyond just stopwords\n",
    "These words should be down-weighted in importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
