{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reguler expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search(\"partern\", text)    # Kiem tra xem partern co trong text hay khong OUI/NON\n",
    "re.findall(\"partern\", txt)  # Dua ra list cua cac tu ma chua partern trong text\n",
    "re.search(\"partern\", txt)  # dua ra INDEX (dau tien) ma partern matchs txt\n",
    "re.split(\"\\s\", txt)\n",
    "new_txt = re.sub(\"partern\", \"99\", txt)  # REPLACE all ones matching the partern by 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "result = pattern.match('$17.89')  ---> match kiểm tra từ chỗ đầu tiên không giống như re.search\n",
    "bool(result)\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n",
    "\n",
    "\n",
    "\n",
    "OR =  |\n",
    "group = ()\n",
    "explicit character = []\n",
    "\n",
    "\n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats.')\n",
    "\n",
    "KQ: ['He', 'has', '11', 'cats']\n",
    "re.match('abc', 'abcdef')\n",
    "\n",
    "\n",
    "if bool(pattern.match(icost)) and bool(pattern.match(tef)):\n",
    "icost = icost.replace(\"$\", \"\")\n",
    "tef = tef.replace(\"$\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of word: Khong quan tam de order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer (Raw tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_docs =[\"The fool doth think he is wise,\",\n",
    "\"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()-->fit/transform\n",
    "vect.fit(list_of_docs) \n",
    "bag = vect.transform(list_of_docs)\n",
    "print(bag.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_\n",
    "vect.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac dinh la lowercase tat ca cac ki tu\n",
    "- min_df  #Chi qua tam den cac tu xuat hien trong it nhat min_df documents.\n",
    "- token_pattern # cach chia chi document thanh cac token\n",
    "- ngram_range = (from_a, to_b) # nhom may token lai voi nhau? (from_a,to_b)\n",
    "- stopwos = 'english'|..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=\"english\", ngram_range=(2, 2)).fit(text_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cac tu khong quan trong: \n",
    "    + Stopword\n",
    "    + min_df  || \n",
    "    + tf-idf resclace feature,tf-idf term frequency inverse documet frequency: Pois cua tu do oo document\n",
    "    = Tim tu phan bien cac document ro rang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIM TU GOC: \n",
    "    - NLTK, Stemming,\n",
    "    - SPACY: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Hi there!\")  : cắt nhỏ câu thành LIST từ vựng và punctuaion \n",
    "sent_tokenize(đoạn văn)  --> câu \n",
    "regexp_tokenize() : tokenize a string or document based on a\n",
    "TweetTokenizer()  -->hastag weet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(word_tokenize(\"Hi there!\"))  --> dictinary từ:số lượng count object\n",
    "count.most_common(2) ---> 2 từ phổ biến\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing = lowercase, lấy root word, bỏ stopword\n",
    "\n",
    "from ntlk.corpus import stopwords\n",
    "text = \"\"\"The cat is in the box. The cat likes the box.\n",
    "The box is over the cat.\"\"\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] chỉ lấy alphabet character\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "'I really liked the movie!',\n",
    "'Awesome action scenes, but boring characters.',\n",
    "'The movie was awful! I hate alien films.',\n",
    "'Space is cool! I liked the movie.',\n",
    "'More space films, please!',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "# có thể updqte với new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]\n",
    "      \n",
    " Term frequency - inverse document frequency    \n",
    "      \n",
    "      \n",
    "      Each corpus may have shared words beyond just stopwords\n",
    "These words should be down-weighted in importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
