{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. modèles linéaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Régression logistique [mặc định đã có L2 Regularization]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\"\"\" Params:\n",
    "    C = 1.0, qui est l'inverse de la force de régularization\n",
    "    (penalty='L2', solver='lbfg') (mặc định) || (penalty='l1', solver='liblinear')\n",
    "\"\"\"\n",
    "\n",
    "\"logreg.coef_, logreg.intercept_\"\n",
    "\n",
    "logred.fit(X_train, y_train)\n",
    "y_pred = logred.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_coef = logreg.coef_.reshape(-1,)\n",
    "logreg_intercept = logreg.intercept_\n",
    "plt.plot(range(len(X_train.columns)), logreg_coef)\n",
    "plt.xticks(range(len(X_train.columns)), X_train.columns, rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "logreg.decision_function(X_test).shape\n",
    "# == (m,) or (m, n_class) for n_class >=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. SVM Linéaire (dung hing loss): défaut co L2 regu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvc = LinearSVC()   \n",
    "\"\"\"Params:\n",
    "    C=1 l'inverse du force de régularization   \n",
    "\"\"\"\n",
    "linsvc.decision_function(X_test) #confidence scores for samples.\n",
    "#Khong co predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC()\n"
     ]
    }
   ],
   "source": [
    "- nhạy cảm với scalling yêu cầu standarNorme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Naive base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB   # all example points are binary  -->TEXT\n",
    "from sklearn.naive_bayes import MultinomialNB # all example points are dénombrement de qqch  -->TEXT\n",
    "from sklearn.naive_bayes import GaussianNB  # all example points are continuous\n",
    "\n",
    "\"\"\"\n",
    "    Params:\n",
    "    alpha : qui contrôle la complexité du modèle, et par conséquence la régularité du frontière de décision\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considère les caractéristiques sont indépendants et traiter chaque caractérítique indépendement \n",
    "    --> rapide-->bon pour les données de grande dimention commment TEXT\n",
    "- La performance de génération est moins bonne que LogReg et LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Les k plus proches voisins (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For finding best n_neighbors\n",
    "neighbors = np.arange(2,20)\n",
    "train_scores =[]\n",
    "test_scores =[]\n",
    "for i in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    test_scores.append(knn.score(X_test, y_test))\n",
    "    \n",
    "plt.plot(train_scores, label=\"train score\")\n",
    "plt.plot(test_scores, label=\"test_score\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"Score of train and test sets with hyperparameters\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SVM à noyau (Kernel SVM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[machines à vecteurs de support = séparateur à vaste marge]**\n",
    "1. l'astuce deu noyau: transformer des caractéristiques d'orgine de manière non) linéaire pour obtenir des nouveaux caractéristique:  par estimer la distance des points de données  \n",
    "2. Puis appliquer un modèle linéaire sur les données avec des nouveau caractéristiques\n",
    "3. Prediction: \n",
    "        - Les échantillons qui se trouvent près de la frontière de décision entre les classes sont importants et s'appellent des vecteur supports\n",
    "        a. mesurer la distance de nouveau example avec des vecteur supports\n",
    "        b. Une décision est éffectué à partir de ces distances et l'importance de ces vecteurs supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\"\"\"params:\n",
    "    C=1.0 # tham số ngược của  régularization\n",
    "    (kernel='rbf', gamma='scale')\n",
    "        # l'inverse du largeur du noyau gaussien = tầm ảnh hưởng của mỗi échangtillon. \n",
    "        gamma bé, tầm ảnh hưởng lớn, các examples giống nhau, frontière de décision trơn. Cần phải xem nhiều điểm để\n",
    "        construct the hyperplane.\n",
    "        Gamma (gamma, default gamma='auto' which uses 1/n_features): \n",
    "    (kernel = 'poly', d, coef0 r) : more in  NLP.  d=2 (quadratic) is common\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** \n",
    "- Tốt cho Dữ liệu rất ít chiều or rất chiều chiều\n",
    "- Puissant pour les jeux de données de taille moyenne avec des caractéristiques dont le sens est similaire\n",
    "\n",
    "Yếu: SVM très senstive au scale. Les données nên có giá trị 0, 1. Ta dùng MinMaxScaler\n",
    "Dữ liệu lớn không hiệu quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- larger gamma leads to more complex boundaries\n",
    "- smaller gamma leads to smoother boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic regression:\n",
    "Is a linear classi,er\n",
    "Can use with kernels, but\n",
    "slow\n",
    "Outputs meaningful\n",
    "probabilities\n",
    "Can be extended to multiclass\n",
    "All data points a(ect ,t\n",
    "L2 or L1 regularization\n",
    "\n",
    "                  \n",
    "                  Support vector machine\n",
    "(SVM):\n",
    "Is a linear classi,er\n",
    "Can use with kernels, and\n",
    "fast\n",
    "Does not naturally output\n",
    "probabilities\n",
    "Can be extended to multiclass\n",
    "Only \"support vectors\"\n",
    "a(ect ,t\n",
    "Conventionally just L2\n",
    "regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tree và các thứ khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=123)\n",
    "\"\"\"Params:\n",
    "    criterion='gini'/'entropy'\n",
    "    max_depth=None, max_features=None, max_leaf_nodes=None,  : Complexité du TREE\n",
    "    random_state=xxx\n",
    "\"\"\"\n",
    "co predict_proba va decision_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                rounded=True,\n",
    "                                filled=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "graph.write_pdf(\"tree.pdf\")   # to pdf\n",
    "graph.write_png(\"tree_png.png\")    # to png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "- Không cần scale, Dễ interpreter (xem cây được), có hàm feature_important \n",
    "- Dễ overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Random Forest : VOTING SOUPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "\"\"\"Params:\n",
    "    n_estimators=Số lượng cây\n",
    "    random_state=None,\n",
    "\n",
    "    criterion='gini', \n",
    "    max_depth=None, max_leaf_nodes=None \n",
    "    max_features='auto', \n",
    "    \n",
    "\"\"\"\n",
    "predict_proba(X) Not Decision_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Không cần scalling, có tính chất feature_important, tránh được overfitting\n",
    "- Nhiều features thì không hiệu quả (vd TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Gradient Bossting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=xxx, learning_rate = xxx,random_state=xxx)\n",
    "\n",
    "\"\"\"Params:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Có loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c2. XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic',  n_estimators=10, seed=123)\n",
    "\n",
    "\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)  #Data Structure for XGBOOST, phai dung cai nay trong CV\n",
    "params={\"objective\":\"binary:logistic\"/'reg:linear',\"max_depth\":4}\n",
    "params[\"lambda\"] -->L2 regu\n",
    "params[\"alpha\"] --->L1 regu\n",
    "tuned_params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1, 'max_depth': 5}\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4, num_boost_round=10, metrics=\"error\"/'auc'>>'mae'/'rmse', as_pandas=True)\n",
    "#num_boost_round = so luong TREE su dung\n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "\n",
    "De control num_boost_round=50, ta co ther dung early_stopping_rounds=xxx\n",
    " if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Turning Parrams Common tree tunable parameters\n",
    "learning rate: learning rate/eta\n",
    "gamma: min loss reduction to create new tree split\n",
    "lambda: L2 reg on leaf weights\n",
    "alpha: L1 reg on leaf weights\n",
    "max_depth: max depth per tree\n",
    "subsample: % samples used per tree\n",
    "colsample_bytree: % features used per tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression 1\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", ...)  #SKLEARN API\n",
    "xg_reg.fit/predict\n",
    "# DUNG TREE AS BASSE LEARNER KHONG CAN BOOSTER\n",
    "\n",
    "#Regression 2: LINEAR BASE LERANER, CAI NAY KHONG PHO BIEN \n",
    "DM_train = xgb.DMatrix(data=X_train,label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test,label=y_test)\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "xg_reg2 = xgb.train(params = params, dtrain=DM_train, num_boost_round=10)\n",
    "\n",
    "xg_cl/_rg.fit/predict\n",
    "xg_reg2.train/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRIDSEARCH XXXXXX EXPONENT TIME\n",
    "\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "gbm_param_grid = {'learning_rate': [0.01,0.1,0.5,0.9],\n",
    "'n_estimators': [200],\n",
    "'subsample': [0.3, 0.5, 0.9]}\n",
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm,param_grid=gbm_param_grid,\n",
    "scoring='neg_mean_squared_error', cv=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random search: review\n",
    "Create a (possibly in,nite) range of hyperparameter values\n",
    "per hyperparameter that you would like to search over\n",
    "Set the number of iterations you would like for the random\n",
    "search to continue\n",
    "During each iteration, randomly draw a value in the range of\n",
    "speci,ed values for each hyperparameter searched over and\n",
    "train/evaluate a model with those hyperparameters\n",
    "A/er you've reached the maximum number of iterations,\n",
    "select the hyperparameter con,guration with the best\n",
    "evaluated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomizedSearchCV(n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One-vs-rest:\n",
    ",t a binary classi,er for\n",
    "each class\n",
    "predict with all, take largest\n",
    "output\n",
    "pro: simple, modular\n",
    "con: not directly optimizing\n",
    "accuracy\n",
    "common for SVMs as well\n",
    "can produce probabilities\n",
    "\"Multinomial\" or \"so/max\":\n",
    ",t a single classi,er for all\n",
    "classes\n",
    "prediction directly outputs\n",
    "best class\n",
    "con: more complicated, new\n",
    "code\n",
    "pro: tackle the problem\n",
    "directly\n",
    "possible for SVMs, but less\n",
    "common\n",
    "can produce probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
